## **Activation Function**

An activation function is a mathematical equation that determines if a neuron should be activated in a neural network: 

**Calculates the weighted sum of inputs**: The function calculates the weighted sum of the inputs.

**Applies a transformation**: The function applies a specific transformation to the weighted sum of the inputs.

Activation functions are used to create non-linear variations in neural networks. They help the system learn and execute difficult tasks, making the neural network more powerful. 

**Here are some examples of activation functions:**

**Tanh (Hyperbolic Tangent)**

This function outputs values between -1 and 1. It's often used in recurrent neural networks (RNNs) and long short-term memory networks (LSTMs). 

**Sigmoid**

This function uses a real value as an input and generates another value between 0 and 1 as the output. It's widely used in classification problems. 

**Linear**

This function is also referred to as "no activation" or "identity function". It's a function where the activation is directly proportional to the input. 
**Softmax**

This function is used for multi-class classification problems where class membership is required on more than two class labels. It forces the values of output neurons to take values between zero and one. 

**Binary Step**

This function compares the input value to a threshold value. If the input value is greater than the threshold value, the neuron is activated. 


![image](https://github.com/user-attachments/assets/3e5843a9-7472-43ee-9580-631227408c59)

